{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eldrey/topicos_contemporaneos/blob/main/Part%202%20-%20LLMs/05%20-%20HuggingFace%20Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XnYDlIaHKEe"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5ozoCYtHKEf"
      },
      "source": [
        "# Utilizando Modelos Pré-treinados com HuggingFace\n",
        "\n",
        "Hugging Face é uma plataforma que oferece uma ampla variedade de modelos pré-treinados para tarefas de processamento de linguagem natural (NLP). Usar esses modelos permite que você aplique técnicas avançadas de NLP sem precisar treinar um modelo do zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDPE4LVMHKEf"
      },
      "source": [
        "## Tokenizadores\n",
        "\n",
        "Tokenização é o processo de converter texto em tokens, que são as unidades básicas que os modelos de NLP processam. A Hugging Face fornece tokenizadores para diferentes modelos, que geram tokens compatíveis com o modelo que será utilizado.\n",
        "\n",
        "Nesta seção, aprenderemos como carregar e utilizar tokenizadores com Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLMRuNj6HKEf",
        "outputId": "6fadacf3-62f8-413d-e6cf-2b2a70b5d6eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408,
          "referenced_widgets": [
            "db27705c42734971a43542fb48b409d0",
            "1f6d8af277b04090af1b325420f1a824",
            "4c80e564a0a543b4a6d94f5e759c31bb",
            "3079571419f343579ae314a8442c02a3",
            "65dddaf68c6f490b97ec4f43ac7e200e",
            "42a27cf0145d4b16a3637dfcb1c24ef4",
            "22bb5cf84c384de0874eb6a9ffdcae1e",
            "789f547095074b2c9647602e30aa4b4a",
            "57c90abaa78644b388503e99f2947396",
            "8fc038af2c1f4003a0f0b63a03bcf0c3",
            "8864adbeaf3a4b528e78b72b1df86666",
            "c94021711c03435c885ad079b3deb679",
            "f185345155d947f3ab71aad95c64602d",
            "be2c2353ea7542c8a57a29de09e743fa",
            "4af57417fd884750b9727d45aa18218b",
            "375efb5ef0794265b5be410e76e6c29f",
            "6404b46b813644ceb13b8362037b1d97",
            "b8f6a63954f54c55b4709567f2de8241",
            "58c85ab42ed149a68844ba1472516517",
            "665e6937c504415aaf71d7665ee19ad7",
            "e31150cecd6447e8baf49e7b5ad50ecf",
            "df43e44076c94ba3b9be815bb28ca368",
            "1a8ae22c2fa840cf8ab59806f870a68c",
            "e0f92f900a844420bdc173b50886b681",
            "60ae153b045c42e6af94fcb484fd702d",
            "28530a63258a4408b3ae43726a7777ac",
            "dc08464d5fde468dbd507fa774fcbde7",
            "8b647b5933494e35bb96c3e8f97d291a",
            "29edca7ff3894cd9bdb5f6f9b2a3de9c",
            "c8891cc5406f48f08ff2e7c436844c39",
            "c59cb63c68624fe79b1ab99d99e91e0d",
            "c569d5075ae94505b863909f2c5f8675",
            "77c63884359e4097ac9584ae90d2e430",
            "701f4df02eb643a79954a33d235afa79",
            "5f97214b47604068bb21ed0be8c948af",
            "4b2602c6e8d0478e9480cd5af23fa05e",
            "e21d74c5322d4cfcb899e0874583a4e6",
            "d45f1455095b4a908011c2eb8d95b602",
            "0642e6c8a8ef4cada95aaa89c18e4334",
            "672e42608a0a4e0b8fe71ed20f8c061b",
            "68fcbedaadb648b69609d7b9aa36f4b2",
            "723e4337a82041bca5154af7d9c470a3",
            "c1d63562ad164f28938ec6eae8f46824",
            "ad402195f3a245f5a0fbbc5751c54e65",
            "382289698a28481b8f75aa9ea7d81d82",
            "c15506624d34477d8b7b63fb73d5a965",
            "e1113997cbc04aec8b39ec603ccd280f",
            "0fe5862651e94519b78f8139db42cce3",
            "faf45914d0684cecb6e1598ad4ae4240",
            "aa7e5599f6594d0a9255c447cea27954",
            "a41d94bfc8b74b388e166d1563fe1096",
            "0cafacd558de49eea97b26388b11cc2d",
            "cefb49955e3a4525b465ec8b554dcba5",
            "206183e2e1ad4b2ab7f8ebb860c28c26",
            "f5934ac8791e41bb98f061d49b37eb31",
            "8db5c1a571d24e5f96bb531482589f77",
            "cbf73229c03349f8898c686297aeb64f",
            "635b50aae0c64539a49d28e8766b39d3",
            "8ea493b54fe745b28eb752675bff4fb0",
            "079f0eed30ea4c6fb9a04a0b07da89a3",
            "ca8f419cab3e4a4583fadd8f3765e51e",
            "94e3f184e4db4421ac68f5662e39e7d0",
            "17a000da374a40269e999417aaf8c62d",
            "5a2658d789064a3cbb6df01a3498b546",
            "a292245a297d4c9d857454e6c541127a",
            "05b57954c025495bad89e92000987f9b",
            "cd0d8bb7fef1457f9a54da3f0490191e",
            "6e91457e87ef4e828d8cdef546ef6eb1",
            "3b34184d823b4ecda2394c7519322030",
            "8c22f71d0d7944f98643f161cd76118e",
            "28bdc634873f4429ba2b7502c1b7113b",
            "77933ce58d0a4007a626501d1a1a1654",
            "0b849bbbd32b4ff3902562b81192ce3c",
            "4edebe77060e49cca1d6fa7953acfc09",
            "4ae2dbc5a405487e84a6f88438d0acc7",
            "18170bfacd5c41ed8ca019223305f14a",
            "aedbf16eb60a44ffa4cc68cd134601db"
          ]
        }
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, BertTokenizer\n",
        "\n",
        "# Carregando tokenizadores para GPT-2 e BERT\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSVJId5OHKEg",
        "outputId": "8649515f-5d28-4bca-cafc-40ec0cc187fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# Exemplo de tokenização\n",
        "input_text = \"Artificial intelligence is the future.\"\n",
        "gpt2_tokens = gpt2_tokenizer.tokenize(input_text)\n",
        "bert_tokens = bert_tokenizer.tokenize(input_text)\n",
        "\n",
        "print(f\"Tokens GPT-2: {gpt2_tokens}\")\n",
        "print(f\"Tokens BERT: {bert_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYHVZ7gDHKEg",
        "outputId": "cb4e7009-6c4e-422d-d439-ba7641bbb4c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "gpt2_ids = gpt2_tokenizer.convert_tokens_to_ids(gpt2_tokens)\n",
        "bert_ids = bert_tokenizer.convert_tokens_to_ids(bert_tokens)\n",
        "\n",
        "print(f\"IDs GPT-2: {gpt2_ids}\")\n",
        "print(f\"IDs BERT: {bert_ids}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMJxkSD3HKEg",
        "outputId": "eba00eee-c0e6-4c66-cdd5-bd74aae14016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "gpt2_input = gpt2_tokenizer(input_text, return_tensors=\"pt\")\n",
        "bert_input = bert_tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"Input GPT-2: {gpt2_input}\")\n",
        "print(f\"Input BERT: {bert_input}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEJURDisHKEg"
      },
      "source": [
        "## Modelos\n",
        "\n",
        "Modelos são as redes neurais que processam os tokens e geram saídas como texto ou embeddings. A Hugging Face disponibiliza uma variedade de modelos, como GPT-2 para geração de texto e BERT para tarefas como busca semântica.\n",
        "\n",
        "Aqui, aprenderemos como carregar e usar esses modelos para diferentes tarefas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO6XxPj4HKEg"
      },
      "source": [
        "### Carregando Modelos Pré-Treinados\n",
        "\n",
        "Para começar, vamos carregar modelos pré-treinados, como o GPT-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuX2iugdHKEh",
        "outputId": "d56ce214-b30b-471d-b504-e182ae6943e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274,
          "referenced_widgets": [
            "9094a275f9d94321b76cfc7521465eb3",
            "bf3b32ae19a74bb7aef5f88d082de3eb",
            "634458e96c164e77ae2012413caed1f0",
            "d53a9cc49bb348f4aa3a181142de1c51",
            "e519cda9278040c09617dd754a531077",
            "63c2dbee1cfb421b8b640934ec30c352",
            "8d29500f765941c8ab070bc942d0a63d",
            "84e7513cd33a4f278febdadef5ece096",
            "20f839978d55454b841588b13bf089d6",
            "86cbb418964648ec997cb696d00f2cbe",
            "4c5bea72db3446fabcd456530cba4693",
            "cb8abaa3e2794f2baf3e94530440cc84",
            "79fc62b3d5d54a6eb8e613263d09f221",
            "5fb6968165d04f57840645aa37d32d6a",
            "d67eea9472f8445aa5a0164a04f97b87",
            "52ec006d74324104bc4f0a2848f37ebf",
            "841a7c742f094e9b9eb6a0a687ccb43a",
            "3e071521545d41b4a161abd5b033463e",
            "8ca4777139e94b5da6a3b07e222eb493",
            "97115847ed4c4c35b84d7ce5bba5d617",
            "370bf8b20aad4278853d7ee79e19703f",
            "9eef363cd2fa4ef0bf52ef2ca452fc5f",
            "8140829e58f24b03ad55cc57854cd031",
            "39e842e8ef194e589f1cdc2f6e335629",
            "1d7b8b26633a4afe9f4e1a1aeabc508d",
            "654064269bd241f1ac042c54ced40723",
            "768f6433e41f479a80e06f1d532d9ef5",
            "c66791cbbf23424a8602ed16b90d00ec",
            "f4f6712db0f24364924eddfc2f919951",
            "657377d6ad1047e8bfcb2f8c378140a0",
            "d3a43565121e448691f4bc84c68828f9",
            "e1abc225b9f346d3b017e50f553896f6",
            "389709c4ab4848ab896dfc05c8368eec",
            "5a447e4619b4400d928679efea95f9e0",
            "d6f098cf0daf4973a0e3ec448fc9f5a2",
            "fcc9cadb49d3491e899eb17895321e43",
            "d513a4655acc49aeba2a0fe17d80f3c0",
            "55c2e509f77c444785a042fc06efb224",
            "6dc834ed7c334244a7fa36918787b52a",
            "8b2a2f5e297640b0a867a8ec1f65b804",
            "2b0c2331c1cb41bc8080a7c092bf79f9",
            "f83d229be2a2463ea3d331feb8b39211",
            "bfd4f3f5e9ae414da9f64a5c4693a509",
            "7b914917e59149ada3a8cafb927039e1"
          ]
        }
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Carregando o tokenizador GPT-2\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Carregando o modelo GPT-2\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=gpt2_tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgnS9SQHHKEh"
      },
      "source": [
        "## Geração de Texto\n",
        "\n",
        "Geração de texto consiste em fornecer uma sequência inicial e permitir que o modelo continue gerando texto a partir dessa entrada. Vamos ver como fazer isso com o GPT-2.\n",
        "\n",
        "A geração de texto é uma das aplicações mais comuns de modelos como o GPT-2, que podem ser usados para completar frases, criar histórias ou mesmo gerar código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_XO28CrHKEh",
        "outputId": "8741f806-33f6-400c-f564-e768cbc09606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# Texto de entrada\n",
        "input_text = \"In a world where AI\"\n",
        "\n",
        "# Tokenizando a entrada\n",
        "input_ids = gpt2_tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"Input IDs: {input_ids}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGGFKT2oHKEi",
        "outputId": "53cab266-329a-4493-f82a-a9a85e96df69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# Gerando o texto\n",
        "output = gpt2_model.generate(**input_ids, max_length=50)\n",
        "\n",
        "print(output.shape)\n",
        "print(f\"Output: {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhZZKK3lHKEi",
        "outputId": "55565543-6250-4b9e-80e5-cc23601586df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# Decodificando o texto gerado\n",
        "generated_text = gpt2_tokenizer.decode(output[0])\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHdEdkeLHKEi"
      },
      "outputs": [],
      "source": [
        "# Função para completar texto\n",
        "def complete_text(input_text, model=gpt2_model, tokenizer=gpt2_tokenizer, max_length=50):\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    output = model.generate(**input_ids, max_length=max_length)\n",
        "    generated_text = tokenizer.decode(output[0])\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTDy4n3yHKEi",
        "outputId": "b34e1a4f-7f1d-447a-8027-86ab139ca694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "prediction = complete_text(\"Brazil is\")\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ug6392PHKEi",
        "outputId": "27384231-9a1c-4799-e765-dc64190b4fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "def generate_next_tokens(input_text, n_tokens=1, model=gpt2_model, tokenizer=gpt2_tokenizer):\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    output = model.generate(input_ids, max_new_tokens=n_tokens)\n",
        "    generated_tokens = output[0][len(input_ids[0]):]  # Extrai apenas os novos tokens gerados\n",
        "    predicted_tokens = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    return predicted_tokens.strip()\n",
        "\n",
        "input_text = \"Brazil is a country. Apple is a fruit. Python is a\"\n",
        "\n",
        "next_token = generate_next_tokens(input_text)\n",
        "print(next_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-lB_WKnHKEi"
      },
      "source": [
        "## Embeddings de Texto\n",
        "\n",
        "Embeddings de texto são representações vetoriais de palavras ou frases que capturam o significado semântico. Esses embeddings podem ser utilizados em várias tarefas de NLP, como classificação de texto, busca semântica e agrupamento.\n",
        "\n",
        "Nesta seção, utilizaremos o BERT para gerar embeddings de texto e visualizar a similaridade entre diferentes frases. Também introduziremos conceitos como similaridade por cosseno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDdgXyGCHKEi",
        "outputId": "db52fbe7-ee85-4284-c4dc-87327465add8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352,
          "referenced_widgets": [
            "483323bfd1954eb294b22e644073ff1d",
            "0631d790d069477f89dae88942df036a",
            "d026dbac9f174535a1da47c47e1bed38",
            "35817a821334404a9b3d044a8c0a5010",
            "ef8fb7a6ff4e414489471e2711e3085b",
            "4f844b24fbb748f69c6b9f771aa60353",
            "1d6208dbe27c4f609432ebca41fbad12",
            "719276601396452f95c5f5ce9b1f13dc",
            "99721d42cda147818cf61c93d46b80fd",
            "0e7d535f025f48e69dae723a2d7a5529",
            "6c76bbdda1af440fa2cd452ee9e3d913",
            "542d527503ec4a58b368aba75ee4778e",
            "cfc0c43ee188433aa47fbde85c9bc276",
            "a071045cc6b7489582ac3002c83ec9ea",
            "31a944ab965845bea7006ede2a50d2f9",
            "846f725f99c342e8be8b3a94ab282740",
            "9763e56b61de4092837d7648499b9477",
            "17ef92dba8b84ffeba46e41228e392dd",
            "c44df7f43c0b4523965e02d1eeec518f",
            "d2b82655b2994af9beee8489368b9f75",
            "fd6bc7555daf4f71ad32159a2dacbebb",
            "d00bdba7327948e1b1035b0c5042b19a",
            "d24e33601c66457396b7b3c12f0b7dee",
            "b2d31973407d4c9d9690f7a02f74a675",
            "4b2c9e4b258446478fb317232717e36b",
            "a46af355ad1c46c797541ae50d0b0d5d",
            "8386fedc75684cda9353f7fc9605b299",
            "de3a90170aaf49b1ba92d63d4aaa3aab",
            "34e6d75385db4df8b74749e8d3e0e345",
            "226cd85e70bf4e3a9c5bfe479cde75d8",
            "e975d92394254d8fa155095b652f8be7",
            "3916c3b4c9bb417e988a13fe058e6d11",
            "5f85aa51515d42498dbe2d39cf44a1de"
          ]
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Carregando o tokenizador BERT\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Carregando o modelo BERT\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlU7SrPKHKEi"
      },
      "source": [
        "## Capturando Embeddings\n",
        "\n",
        "Predição de embeddings consiste em fornecer um texto e utilizar um modelo encoder para prever a representação vetorial deste texto. Vamos ver como fazer isso com o BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXldJhwoHKEi",
        "outputId": "90fac239-4fd7-4eef-c85a-34988989c4e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# Texto de entrada\n",
        "text = \"Artificial intelligence is the future.\"\n",
        "input_ids = bert_tokenizer.encode(text, return_tensors=\"pt\")\n",
        "\n",
        "# Obtendo a representação do texto\n",
        "with torch.no_grad():\n",
        "    output = bert_model(input_ids)\n",
        "\n",
        "\n",
        "pooled_output = output.last_hidden_state.mean(dim=1)\n",
        "\n",
        "print(output.last_hidden_state.shape)\n",
        "print(pooled_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOAU9RpMHKEj"
      },
      "outputs": [],
      "source": [
        "# Função para gerar embeddings usando BERT\n",
        "def get_embedding(text):\n",
        "    input_ids = bert_tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(input_ids)\n",
        "    return outputs.last_hidden_state.mean(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5NxtGi9HKEj"
      },
      "source": [
        "### Medidas de Similaridade\n",
        "\n",
        "A similaridade por cosseno é uma métrica comum utilizada para medir a similaridade entre dois vetores no espaço de embeddings. Ela calcula o cosseno do ângulo entre dois vetores, onde um valor de 1 indica vetores idênticos e um valor de 0 indica vetores ortogonais (sem similaridade).\n",
        "\n",
        "A fórmula da similaridade por cosseno é:\n",
        "\n",
        "$\n",
        "\\text{similaridade}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
        "$\n",
        "\n",
        "Vamos calcular a similaridade entre diferentes frases usando embeddings gerados por BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z09otb60HKEj"
      },
      "outputs": [],
      "source": [
        "# Exemplo de frases\n",
        "query = \"Artificial intelligence is transforming industries.\"\n",
        "doc1 = \"AI is changing the way we work.\"\n",
        "doc2 = \"Brazil is a country in South America.\"\n",
        "\n",
        "# Gerando embeddings\n",
        "query_embedding = get_embedding(query)\n",
        "doc1_embedding = get_embedding(doc1)\n",
        "doc2_embedding = get_embedding(doc2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SQ4UU5ZHKEj",
        "outputId": "c60d66c4-cb47-4b40-d17a-2d73b2c07c39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# Calculando similaridade por cosseno\n",
        "cos = torch.nn.CosineSimilarity(dim=1)\n",
        "similarity_doc1 = cos(query_embedding, doc1_embedding)\n",
        "similarity_doc2 = cos(query_embedding, doc2_embedding)\n",
        "\n",
        "print(f\"Similaridade com doc1: {similarity_doc1.item():.4f}\")\n",
        "print(f\"Similaridade com doc2: {similarity_doc2.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGu5i-bnHKEj"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tepg-1zpHKEj"
      },
      "source": [
        "### Exercício 1\n",
        "Utilizando GPT-2, crie uma função que preveja o sujeito em uma frase. Por exemplo: Em \"John went to the store\", o sujeito é John."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz7TFJvgHKEj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxA5mTprHKEj"
      },
      "source": [
        "### Exercício 2\n",
        "Em um sistema avançado de busca por livros, você deverá implementar uma função que faça uma busca semântica e retorne os 5 livros mais apropriados de acordo com a consulta do usuário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Sex4H9WHKEj"
      },
      "outputs": [],
      "source": [
        "descriptions = [\n",
        "    \"A tale of love and loss set against the backdrop of war.\",\n",
        "    \"A gripping mystery where nothing is as it seems.\",\n",
        "    \"An epic fantasy adventure in a world of magic and dragons.\",\n",
        "    \"A heartwarming story of friendship and second chances.\",\n",
        "    \"A chilling thriller that will keep you on the edge of your seat.\",\n",
        "    \"A coming-of-age story about finding yourself and your place in the world.\",\n",
        "    \"A historical novel that brings the past to life with vivid detail.\",\n",
        "    \"A suspenseful crime novel where the detective becomes the hunted.\",\n",
        "    \"A dystopian future where one woman's rebellion could change everything.\",\n",
        "    \"A romantic comedy that will make you believe in love again.\",\n",
        "    \"A science fiction saga that explores the limits of human ingenuity.\",\n",
        "    \"A powerful drama about family, secrets, and redemption.\",\n",
        "    \"A journey through time to uncover hidden truths.\",\n",
        "    \"A modern fairy tale where dreams really do come true.\",\n",
        "    \"A dark fantasy filled with intrigue, betrayal, and forbidden magic.\",\n",
        "    \"A psychological thriller that will mess with your mind.\",\n",
        "    \"A poetic exploration of life, love, and everything in between.\",\n",
        "    \"A detective novel where every clue leads to more questions.\",\n",
        "    \"A story of survival and the strength of the human spirit.\",\n",
        "    \"A heart-pounding adventure in a world beyond our own.\",\n",
        "    \"A memoir of a life lived on the edge of society.\",\n",
        "    \"A romance that defies the boundaries of time and space.\",\n",
        "    \"A political thriller set in a world of corruption and power.\",\n",
        "    \"A fantasy epic that weaves together destiny and desire.\",\n",
        "    \"A mystery novel where the past refuses to stay buried.\",\n",
        "    \"A heartwrenching story of love, loss, and letting go.\",\n",
        "    \"A darkly comic tale of life in the absurd.\",\n",
        "    \"A science fiction adventure that questions what it means to be human.\",\n",
        "    \"A historical romance set in a time of revolution and change.\",\n",
        "    \"A supernatural thriller where nightmares come to life.\",\n",
        "    \"A journey of self-discovery in a world that demands conformity.\",\n",
        "    \"A story of forbidden love in a society bound by tradition.\",\n",
        "    \"A fast-paced action novel where every second counts.\",\n",
        "    \"A lyrical exploration of nature, solitude, and the passage of time.\",\n",
        "    \"A detective story where the truth is stranger than fiction.\",\n",
        "    \"A powerful saga of family, loyalty, and betrayal.\",\n",
        "    \"A fantastical journey through a land of myths and legends.\",\n",
        "    \"A tale of revenge, justice, and the price of power.\",\n",
        "    \"A story of hope in the face of overwhelming odds.\",\n",
        "    \"A quirky romance where opposites truly attract.\",\n",
        "    \"A sci-fi thriller that blurs the line between reality and illusion.\",\n",
        "    \"A historical epic that spans generations and continents.\",\n",
        "    \"A crime novel where the line between right and wrong is razor-thin.\",\n",
        "    \"A love story that unfolds in the most unexpected way.\",\n",
        "    \"A philosophical exploration of what it means to live a good life.\",\n",
        "    \"A gripping tale of survival in a post-apocalyptic world.\",\n",
        "    \"A romance that blossoms in the midst of chaos and war.\",\n",
        "    \"A detective novel that unravels the darkest secrets of the human soul.\",\n",
        "    \"A story of redemption and the power of forgiveness.\",\n",
        "    \"A fantasy adventure where a reluctant hero must save the world.\"\n",
        "]\n",
        "\n",
        "input_text = \"A horror novel\"\n",
        "\n",
        "# ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtPrE7nXHKEj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}