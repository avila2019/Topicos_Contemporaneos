{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d_a-AY7BHwE4"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "# https://platform.openai.com/settings/organization/api-keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKTgJzqZHwE4"
      },
      "source": [
        "### Modelos\n",
        "\n",
        "LangChain permite o uso de diferentes modelos de linguagem, tanto de c√≥digo aberto quanto comerciais (como OpenAI, Cohere, Anthropic, etc.).\n",
        "\n",
        "Os mais comuns:\n",
        "\n",
        "- `ChatOpenAI`: para modelos de chat da OpenAI (`gpt-3.5-turbo`, `gpt-4`, etc.)\n",
        "- `OpenAI`: para modelos n√£o conversacionais (completions)\n",
        "- `HuggingFaceHub`: para modelos hospedados no Hugging Face\n",
        "- `ChatAnthropic`, `ChatCohere`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "maEJkdYoHwE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618b616f-ee25-42bd-f666-6300ce91c77d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.10)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.13 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.2.13)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.21.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (26.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (9.1.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.67.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=2.20.0->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain-openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.13->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-openai\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi0_m8V9HwE5",
        "outputId": "1e8760db-a0e8-4ae7-c801-40855edca9bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Ol√°! Como posso ajudar voc√™ hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_373a14eb6f', 'id': 'chatcmpl-DAn854S0zkt7Bh6suAgBsJLx9PlZd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7383-3440-7e53-afd5-069a507d2eba-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Ol√°\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8VxStrdHwE5"
      },
      "source": [
        "## Prompts\n",
        "\n",
        "O prompt √© o texto de entrada enviado ao modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjwRMDduHwE5"
      },
      "source": [
        "### Templates Simples\n",
        "\n",
        "Usamos `PromptTemplate` para criar templates reutiliz√°veis com vari√°veis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyRtJCs6HwE5",
        "outputId": "9c526f57-d8dd-4e25-877b-0cdd4b2ee1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para portugu√™s: Artificial Intelligence is the future!', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para portugu√™s: {texto}\")\n",
        "\n",
        "prompt = prompt_template.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV7PZ7E3HwE5",
        "outputId": "f16e28af-ad72-458f-9a71-64e92446e001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intelig√™ncia Artificial √© o futuro!\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Z8jUvmHwE6"
      },
      "source": [
        "### Templates de Chat\n",
        "\n",
        "Para modelos de chat, usamos `ChatPromptTemplate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MydyaKR2HwE6",
        "outputId": "d6914894-0419-4374-d529-e23385ff7d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Ol√°, como voc√™ est√°?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 36, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_373a14eb6f', 'id': 'chatcmpl-DAn9CCJgNzwAnsNKNVb3JJv5os1wj', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7384-431c-79d2-8f0d-686d966904db-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xcOCkKjMHwE6"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Voc√™ √© um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwH3X4YEHwE6",
        "outputId": "924b92f5-d449-427e-e848-dccdfca1ef40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"ingl√™s\",\n",
        "    \"lingua_destino\": \"portugu√™s\",\n",
        "    \"texto\": \"Hello, how are you?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtURheGpHwE6",
        "outputId": "67d24240-d5ba-426b-8337-ad19bac4630e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ol√°, como voc√™ est√°?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x033Uz-SHwE6"
      },
      "source": [
        "## Output Parsers\n",
        "\n",
        "Os **parsers de sa√≠da** transformam a resposta do LLM em formatos √∫teis: JSON, dicion√°rios, Pydantic, etc.\n",
        "\n",
        "Eles ajudam a garantir que a resposta seja estruturada corretamente, facilitando a integra√ß√£o com c√≥digo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0SKlLD_HwE7",
        "outputId": "5a4c9cd4-5001-46b0-a0be-7487a3ee4f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='A capital do Rio Grande do Norte √© Natal.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_373a14eb6f', 'id': 'chatcmpl-DAn9wL9b951vXhS0kEtLioQIQPTwC', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7384-f9dd-7871-9095-00c5d5998b74-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Sa√≠da do parser:\n",
            "A capital do Rio Grande do Norte √© Natal.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "str_parser = StrOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Qual a capital do Rio Grande do Norte?\")\n",
        "output = str_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Sa√≠da do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guYXpNx6HwE7",
        "outputId": "1a4f6277-3dac-44f8-de0c-8c2fc7b7c21e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='```json\\n{\\n    \"pr√≥ton\": {\\n        \"massa\": 1.6726e-27,\\n        \"carga\": \"+1e\"\\n    },\\n    \"n√™utron\": {\\n        \"massa\": 1.6750e-27,\\n        \"carga\": \"0\"\\n    },\\n    \"el√©tron\": {\\n        \"massa\": 9.1094e-31,\\n        \"carga\": \"-1e\"\\n    }\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 38, 'total_tokens': 136, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_373a14eb6f', 'id': 'chatcmpl-DAnAdcJq4S7sOfNRlgLNg9s1b1QYJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7385-a0ea-7cc3-a2e7-e961db08fa73-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 38, 'output_tokens': 98, 'total_tokens': 136, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Sa√≠da do parser:\n",
            "{'pr√≥ton': {'massa': 1.6726e-27, 'carga': '+1e'}, 'n√™utron': {'massa': 1.675e-27, 'carga': '0'}, 'el√©tron': {'massa': 9.1094e-31, 'carga': '-1e'}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Quais as massas e cargas das part√≠culas que constituem o √°tomo? Responda no formato JSON em que cada chave seja o nome da part√≠cula\")\n",
        "output = json_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Sa√≠da do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMS7TanmHwE7"
      },
      "source": [
        "## üîó Encadeamento\n",
        "\n",
        "LangChain permite encadear componentes usando o operador `|`, formando um fluxo simples e modular.\n",
        "\n",
        "O padr√£o mais comum √©:\n",
        "\n",
        "```python\n",
        "PromptTemplate | LLM | OutputParser\n",
        "````\n",
        "\n",
        "Ou seja:\n",
        "\n",
        "* O **prompt** gera o texto com base nas vari√°veis\n",
        "* O **modelo** (LLM) responde ao prompt\n",
        "* O **parser** transforma a resposta em formato estruturado (como dicion√°rio ou objeto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Yed44rEnHwE7"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIBV0i-wHwE7",
        "outputId": "c868aba3-a65b-45b5-d026-17c1eeb824ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬°Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"ingl√™s\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubar√µes!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Cm5EsfQAHwE8"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvU2-IIGHwE8",
        "outputId": "db1b1ec9-3803-4209-8414-0d4e0fe11869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬°Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"ingl√™s\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttv4UmXvHwE8"
      },
      "source": [
        "## Sa√≠da Estruturada\n",
        "\n",
        "Quando usamos modelos de linguagem, muitas vezes queremos que eles retornem respostas em um **formato espec√≠fico**, como um dicion√°rio ou JSON com campos pr√©-definidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Itrl18rPHwE8"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Person(BaseModel):\n",
        "    name: str = Field(description=\"O nome da pessoa presente no texto.\")\n",
        "    age: int = Field(description=\"A idade da pessoa presente no texto.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8rF94hrqHwE8"
      },
      "outputs": [],
      "source": [
        "structured_llm = llm.with_structured_output(Person)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YD2hsHYnHwE8"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"Voc√™ √© um assistente analisa textos e extrai informa√ß√µes sobre pessoas. Um texto ser√° enviado e voc√™ deve extrair o nome e a idade da pessoa mencionada.\n",
        "\n",
        "Texto: {text}\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(SYSTEM_PROMPT)\n",
        "\n",
        "chain = prompt_template | structured_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YGB8zDEHwE8",
        "outputId": "ac43c05d-f52f-4217-e39a-7557fc8e5690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Jo√£o' age=30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='parsed', input_value=Person(name='Jo√£o', age=30), input_type=Person])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\"text\": \"Certo dia Jo√£o foi ao mercado e comprou um bolo para comemorar seu trig√©simo anivers√°rio.\"})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxia5UGPHwE8"
      },
      "source": [
        "## Exerc√≠cios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhA4izITHwE8"
      },
      "source": [
        "### Exerc√≠cio 1\n",
        "Crie uma `chain` que a partir de um t√≥pico informado pelo usu√°rio, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "B1k9sfOMHwE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "714d2670-6dae-4a7f-9d72-6ab04714f074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por que os programadores sempre confundem Halloween com o Natal?\n",
            "\n",
            "Porque OCT 31 √© igual a DEC 25!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_piadas = ChatPromptTemplate.from_template(\n",
        "    \"Crie uma piada engra√ßada sobre o seguinte t√≥pico: {topico}\"\n",
        ")\n",
        "\n",
        "chain_piadas = prompt_piadas | llm\n",
        "\n",
        "resposta = chain_piadas.invoke({\"topico\": \"programa√ß√£o\"})\n",
        "print(resposta.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwIdWg4rHwE8"
      },
      "source": [
        "### Exerc√≠cio 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EbQTkz-JHwE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9309f64b-f686-4bbe-adda-1cab1eeec9dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O sentimento do texto √© positivo.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_sentimento = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Classifique o sentimento do texto abaixo como:\n",
        "    positivo, neutro ou negativo.\n",
        "\n",
        "    Texto: {texto}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Using the modern chaining syntax (Prompt | LLM)\n",
        "chain_sentimento = prompt_sentimento | llm\n",
        "\n",
        "resposta = chain_sentimento.invoke({\n",
        "    \"texto\": \"Hoje foi um dia maravilhoso, estou muito feliz!\"\n",
        "})\n",
        "\n",
        "print(resposta.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2HBe9-7HwE8"
      },
      "source": [
        "### Exerc√≠cio 3\n",
        "Crie uma `chain` que gere o c√≥digo de uma fun√ß√£o Python de acordo com a descri√ß√£o do usu√°rio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PeBl1TU0HwE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ecab41-4ece-4929-964e-7dad7e2b71a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def calcular_media(numeros):\n",
            "    if not numeros:\n",
            "        return 0\n",
            "    return sum(numeros) / len(numeros)\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_codigo = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Gere o c√≥digo de uma fun√ß√£o Python conforme a descri√ß√£o abaixo:\n",
        "\n",
        "    {descricao}\n",
        "\n",
        "    Retorne apenas o c√≥digo.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "chain_codigo = prompt_codigo | llm\n",
        "\n",
        "resposta = chain_codigo.invoke({\n",
        "    \"descricao\": \"Fun√ß√£o que calcula a m√©dia de uma lista de n√∫meros\"\n",
        "})\n",
        "\n",
        "print(resposta.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UYMgXgcHwE8"
      },
      "source": [
        "### Exerc√≠cio 4\n",
        "Crie uma `chain` que explique de forma simplificada um t√≥pico geral fornecido pelo usu√°rio e, em seguida, traduza a explica√ß√£o para ingl√™s. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3W--aZ8KHwE8"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-openai\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] =(\"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\")\n"
      ],
      "metadata": {
        "id": "Lfz3hjFNkkeP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.3\n",
        ")\n"
      ],
      "metadata": {
        "id": "xYKsv4Xek-gq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Template 1 - Explica√ß√£o\n",
        "prompt_explicacao = ChatPromptTemplate.from_template(\n",
        "    \"Explique de forma simples o seguinte t√≥pico: {topico}\"\n",
        ")\n",
        "\n",
        "# Template 2 - Tradu√ß√£o\n",
        "prompt_traducao = ChatPromptTemplate.from_template(\n",
        "    \"Traduza o texto abaixo para ingl√™s:\\n\\n{explicacao}\"\n",
        ")\n",
        "\n",
        "# Encadeamento moderno (LCEL)\n",
        "chain = (\n",
        "    prompt_explicacao\n",
        "    | llm\n",
        "    | (lambda x: {\"explicacao\": x.content})\n",
        "    | prompt_traducao\n",
        "    | llm\n",
        ")\n",
        "\n",
        "resultado = chain.invoke({\"topico\": \"Intelig√™ncia Artificial\"})\n",
        "\n",
        "print(resultado.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EmADZf6lJTD",
        "outputId": "642735d0-d404-4e9f-d4a6-1e9ba8504e77"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial Intelligence (AI) is a branch of computer science dedicated to creating systems and programs that can perform tasks that typically require human intelligence. This includes activities such as learning, reasoning, problem-solving, understanding language, and recognizing patterns.\n",
            "\n",
            "In simple terms, AI aims to make machines \"think\" and \"act\" like humans. For example, virtual assistants like Siri or Alexa use AI to understand voice commands and answer questions. Another example is recommendation systems, such as those that suggest movies or music based on what you have already watched or listened to.\n",
            "\n",
            "There are different types of AI, ranging from the simplest, which follow specific rules, to the more advanced, which use machine learning to improve their performance over time. AI is present in many areas, such as healthcare, transportation, finance, and entertainment, and has the potential to transform the way we live and work.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAk_OzTEHwE9"
      },
      "source": [
        "### Exerc√≠cio 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eu tive que atualizar pois estava dando erro\n",
        "\n",
        "!pip install --upgrade langchain langchain-community langchain-experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc252HOMlhri",
        "outputId": "33f7ba74-92e6-4e9c-c43b-6a7231dedccb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.13)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.7.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.14.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (0.3.5)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain) (1.12.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TE_JHzZyHwE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c0ce49-1f45-4737-9099-5cd2605f613a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O CESAR School est√° localizado em Recife, Pernambuco, Brasil. A sede fica no bairro de Boa Viagem.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-openai\n",
        "\n",
        "\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_cesar = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Voc√™ √© um assistente especialista em informa√ß√µes sobre o CESAR School.\n",
        "    Responda a pergunta do usu√°rio de forma clara e objetiva.\n",
        "\n",
        "    Pergunta: {pergunta}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "chain_cesar = prompt_cesar | llm\n",
        "\n",
        "resposta = chain_cesar.invoke({\n",
        "    \"pergunta\": \"Onde fica o CESAR School?\"\n",
        "})\n",
        "\n",
        "print(resposta.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}